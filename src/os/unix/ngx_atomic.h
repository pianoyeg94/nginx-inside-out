#ifndef _NGX_ATOMIC_H_INCLUDED_
#define _NGX_ATOMIC_H_INCLUDED_

#include <ngx_config.h>
#include <ngx_core.h>

#if (NGX_HAVE_LIBATOMIC)
    
    /* TODO!!!!! */


#elif (NGX_HAVE_GCC_ATOMIC) /* autogenerated depending on GCC version >= 4.1 */

/* GCC 4.1 builtin atomic operations */

/**
 * Built-in GCC functions for atomic memory access
 * ------------------------------------------------
 * The following builtins are intended to be compatible with those 
 * described in the Intel Itanium Processor-specific Application 
 * Binary Interface, section 7.4. As such, they depart from the 
 * normal GCC practice of using the "__builtin_" prefix, and further 
 * that they are overloaded such that they work on multiple types.
 * 
 * The definition given in the Intel documentation allows only for 
 * the use of the types int, long, long long as well as their unsigned 
 * counterparts. GCC will allow any integral scalar or pointer type that 
 * is 1, 2, 4 or 8 bytes in length.
 * 
 * Not all operations are supported by all target processors. If a particular 
 * operation cannot be implemented on the target processor, a warning will 
 * be generated and a call to an external function will be generated. The 
 * external function will carry the same name as the builtin, with an additional 
 * suffix `_n' where n is the size of the data type.
 * 
 * In most cases, these builtins are considered a full barrier. That is, no memory 
 * operand will be moved across the operation, either forward or backward. Further, 
 * instructions will be issued as necessary to prevent the processor from speculating 
 * loads across the operation and from queuing stores after the operation.
 * 
 * All of the routines are are described in the Intel documentation to take "an optional 
 * list of variables protected by the memory barrier". It's not clear what is meant by that; 
 * it could mean that only the following variables are protected, or it could mean that 
 * these variables should in addition be protected. At present GCC ignores this list and protects 
 * all variables which are globally accessible. If in the future we make some use of this list, 
 * an empty list will continue to mean all globally accessible variables.
 * 
 * type __sync_fetch_and_add (type *ptr, type value, ...)
 * type __sync_fetch_and_sub (type *ptr, type value, ...)
 * type __sync_fetch_and_or (type *ptr, type value, ...)
 * type __sync_fetch_and_and (type *ptr, type value, ...)
 * type __sync_fetch_and_xor (type *ptr, type value, ...)
 * type __sync_fetch_and_nand (type *ptr, type value, ...)
 *       These builtins perform the operation suggested by the name, and returns the value that had previously been in memory. That is,
 * 
 *                 { tmp = *ptr; *ptr op= value; return tmp; }
 *                 { tmp = *ptr; *ptr = ~tmp & value; return tmp; }   // nand  
 * 
 * type __sync_add_and_fetch (type *ptr, type value, ...)
 * type __sync_sub_and_fetch (type *ptr, type value, ...)
 * type __sync_or_and_fetch (type *ptr, type value, ...)
 * type __sync_and_and_fetch (type *ptr, type value, ...)
 * type __sync_xor_and_fetch (type *ptr, type value, ...)
 * type __sync_nand_and_fetch (type *ptr, type value, ...)
 *       These builtins perform the operation suggested by the name, and return the new value. That is,
 * 
 *                 { *ptr op= value; return *ptr; }
 *                 { *ptr = ~*ptr & value; return *ptr; }   // nand
 * 
 * bool __sync_bool_compare_and_swap (type *ptr, type oldval, type newval, ...)
 * type __sync_val_compare_and_swap (type *ptr, type oldval, type newval, ...)
 *       These builtins perform an atomic compare and swap. That is, if the current value of *ptr is oldval, then write newval into *ptr.
 * 
 *       The "bool" version returns true if the comparison is successful and newval was written. The "val" version returns the contents 
 *       of *ptr before the operation.
 * 
 * __sync_synchronize (...)
 *   This builtin issues a full memory barrier.
 * 
 * type __sync_lock_test_and_set (type *ptr, type value, ...)
 *       This builtin, as described by Intel, is not a traditional test-and-set operation, but rather an atomic exchange operation. 
 *       It writes value into *ptr, and returns the previous contents of *ptr.
 * 
 *       Many targets have only minimal support for such locks, and do not support a full exchange operation. In this case, a target 
 *       may support reduced functionality here by which the only valid value to store is the immediate constant 1. The exact value 
 *       actually stored in *ptr is implementation defined.
 * 
 *       This builtin is not a full barrier, but rather an acquire barrier. This means that references after the builtin cannot move 
 *       to (or be speculated to) before the builtin, but previous memory stores may not be globally visible yet, and previous memory 
 *       loads may not yet be satisfied.
 * 
 * void __sync_lock_release (type *ptr, ...)
 *       This builtin releases the lock acquired by __sync_lock_test_and_set. Normally this means writing the constant 0 to *ptr.
 *      
 *       This builtin is not a full barrier, but rather a release barrier. This means that all previous memory stores are globally 
 *       visible, and all previous memory loads have been satisfied, but following memory reads are not prevented from being speculated 
 *       to before the barrier.
 */

#define NGX_HAVE_ATOMIC_OPS  1  

typedef long                        ngx_atomic_int_t;
typedef unsigned long               ngx_atomic_uint_t; /* TODO!!!! */

#if (NGX_PTR_SIZE == 8)
#define NGX_ATOMIC_T_LEN            (sizeof("-9223372036854775808") - 1)
#else
#define NGX_ATOMIC_T_LEN            (sizeof("-2147483648") - 1)
#endif

typedef volatile ngx_atomic_uint_t  ngx_atomic_t;


#define ngx_atomic_cmp_set(lock, old, set)                                    \
    __sync_bool_compare_and_swap(lock, old, set)

#define ngx_atomic_fetch_add(value, add)                                      \
    __sync_fetch_and_add(value, add)

#define ngx_memory_barrier()        __sync_synchronize()

#if (__i386__ || __i386 || __amd64__ || __amd64)

/**
 * PAUSE notifies the CPU that this is a spinlock wait loop so memory 
 * and cache accesses may be optimized. PAUSE may actually stop CPU 
 * for some time to save power. Older CPUs decode it as NOP, so you 
 * don't have to check if its supported. Older CPUs will simply do 
 * nothing (NOP).
 * 
 * The intel manual and other information available state that:
 *     The processor uses this hint to avoid the memory order violation in most situations,
 *     which greatly improves processor performance.
 *     For this reason, it is recommended that a PAUSE instruction be placed
 *     in all spin-wait loops. An additional function of the PAUSE instruction
 *     is to reduce the power consumed by Intel processors.
 * 
 * Branch mispredictions and pipeline flushes:
 *     https://stackoverflow.com/questions/4725676/how-does-x86-pause-instruction-work-in-spinlock-and-can-it-be-used-in-other-sc
 * 
 * https://stackoverflow.com/questions/12894078/what-is-the-purpose-of-the-pause-instruction-in-x86
 */

#define ngx_cpu_pause()             __asm__ ("pause")
#else
#define ngx_cpu_pause()
#endif


#endif 

#endif /* _NGX_ATOMIC_H_INCLUDED_ */